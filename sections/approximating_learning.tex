\section{Active Learning via Covering Problems} \label{sec:AL}

% To build our algorithm we will make use of the following definition:
% \begin{definition}[Sepcover]
%     Let $D$ be any decision tree for $\cI=\br{\cH, \cT, \cF}$. We define a sequence of tests $P_D$ called \emph{sepcover} as follows. Initially, $P_D$ is empty and $\cH' = \cH$. While $\spr{\cH'}> \spr{\cH}/2$, we append to $P_D$ the test $r\br{D_{\cH'}}$ and update $\cH'$ to be the set of hypotheses corresponding to the child of $D_{\cH'}$ that contains the most hypotheses. If $\COSTW\br{D}=\OPTW\br{\cI}$, then we denote $\sepcover{\cI} = P_D$ (ties broken arbitrarily).
% \end{definition}

The following definition gives a basic tool for our algorithm.
\begin{definition}[Sepcover]
  Let $D$ be any decision tree for $\cI=\br{\cH, \cT, \cF}$.
  Let $v$ be a node that is closest to the root and has the property that after removal of $\tests{D}{v}$ from $D$, each subtree has at most $\spr{\cH}/2$ hypotheses as potential targets.
  Then the set $\tests{D}{v}$ is called a \emph{sepcover} for $D$ and is denoted by $\sepcover{D}$.
  If $\COSTW\br{D}=\OPTW\br{\cI}$, then we write $\sepcover{\cI} = \sepcover{D}$ (ties broken arbitrarily).
\end{definition}
Note that a sepcover can be efficiently computed for a decision tree $D$ by the following greedy procedure that finds the above node $v$.
Let initially $v$ be the root.
If more that $\spr{\cH}/2$ hypotheses are potential targets after the query to $v$, then find the child $u$ of $v$ that maximizes $\cH_u$ and let $v:=u$.
Otherwise the process is over.
This is indeed correct because each test $x\in\tests{D}{v}$ can have at most one child associated with more than half of the hypotheses from $\cH_x$.

\DD{byc moze przesunac boundy do podrozdzialow ponizej}
We will use sepcovers to lowerbound optimal solutions both for $\ProblemPCWCAL$ and $\ProblemPCACAL$.
Since $P_D$ is a subpath of $D$, we immediately have the following.
\begin{observation} \label{obs:sepcoverWC}
    Let $\cI$ be any instance of $\ProblemPCAL$.
    Then, $\spr{\sepcover{\cI}} \leq \OPTW\br{\cI}$.
\end{observation}
In order to make a statemnt for the average case, we will need the following notion.
For any node $v$ in a decision tree $D$, denote for brevity $S=\tests{D}{v}$ to be all tests done to arrive at $v$ in $D$.
Let
$$
\COSTA\br{S, \cI} = \sum_{t \in S} \spr{\cH_t}.
$$
This allows us to decompose the cost criterion as follows.
\begin{observation} \label{obs:WCdecomposition}
    Let $D$ be any decision tree for an instance $\cI$ of $\ProblemPCACAL$ and let $S=\tests{D}{v}$ for any node $v$ of $D$.
    Then,
    $$
    \COSTA\br{D} = \COSTA\br{S} + \sum_{D'\in D\setminus S} \COSTA\br{D'}.
    $$
%     $$
%     \COSTA\br{D, \cI} = \COSTA\br{S, \cI} + \sum_{D'\in D\setminus S} \COSTA\br{D', \cI}.
%     $$
\end{observation}
Here, $D\setminus S$ is the collection of subtrees (a forest) obtained by the removal of all nodes in $S$ from $D$.
As an immediate corollary, when taking an optimal $D$ and the corresponding $S=\sepcover{\cI}$ we have:
\begin{observation} \label{obs:sepcoverAC}
    Let $\cI$ be any instance of $\ProblemPCACAL$.
    Then, $\COSTA\br{\sepcover{\cI}} \leq \OPTA\br{\cI}$.
\end{observation}
%Figure \ref{fig:sepcover} illustrates the definition of sepcover.
% \begin{figure}[h]
% \centering
% \input{figures/sepcover.tex}
% \caption{Sepcover sequence in a decision tree}
% \label{fig:sepcover}
% \end{figure}

%We will say that a test in $\sepcover{\cI}$ \emph{sepcovers} an element $u \in \cH$ in $C$ if after applying the test $t$ in $C$, $u$ belongs to a response $\cH'$ of size at most $\spr{\cH}/2$.

Consider a set of hypotheses $\cH$ and the corresponding tests $\cT$.
For each test $t\in\cT$
$$
\xi(t)=\brc{h\in\cH \mid h\in U \textup{ for some }U\in t\textup{ s.t. }  \spr{U}\leq\frac{3}{4}\spr{\cU}}.
$$
Then, $\xi(\cT)=\brc{\xi(t)\mid t\in\cT}$ and if $\preceq$ is a partial order on $\cT$, then $\preceq_{\xi}$ is the corresponding partial order on $\xi(\cT)$, i.e., $t\preceq t'$ if and only if $\xi(t)\preceq_{\xi}\xi(t')$.
By extension, for any subset $S\subseteq\cT$, $\xi(S)=\brc{\xi(t) \mid t\in S}$.
In our construction, in order to solve a $\ProblemPCAL$ for an instance $\cI=\br{\cH, \cT, \preceq}$, we will apply an algorithm for $\ProblemPCSC$ with an input $\cI=\br{\cH, \xi(\cT), \preceq_{\xi}, \spr{\cH}/4}$.

\medskip
The ide behind the algorithms for the worst case (Section~\ref{subsection:PCWCAL}) and average case (Section~\ref{subsection:PCACAL}) is similar.
Hence we extract here the common generic subroutine (see Algorithm~\ref{alg:worstDecisionTree}) that will be used in both cases.
The only difference is a black-box procedure that is used to solve a particular subproblem -- either $\ProblemPCSC$ or $\ProblemPCMSSC$.
Algorithm \ref{alg:worstDecisionTree} is recursive and a set cover $S$ is used to split the instance and make recursive calls.
Note that the $S$ is supposed to play the role of $\sepcover{\cI}$ according to the lower bounds in Lemmas~\ref{lemma:cover_sep_worst_case} and~\ref{lemma:cover_sep_average_case} given in the subsections below.
Hence we indeed can afford constructing an arbitrary decision tree $D_S$ on the tests in $S$.
\input{pseudocodes/worst_case_learning.tex}


\subsection{Precedence Constrained Worst Case Active Learning} \label{subsection:PCWCAL}

%We will use $\spr{\sepcover{\cI}}$ as a lower bound on $\OPTW\br{\cI}$ in the analysis of the approximation algorithm for $\ProblemPCWCAL$.
\begin{lemma}\label{lemma:cover_sep_worst_case}
    Let $\cI=\br{\cH, \cT, \preceq}$ be any $\ProblemPCWCAL$ instance.
    Let $S^*$ be an optimal solution to $\ProblemPCSC$ on instance $\br{\cH, \xi(\cT), \preceq_{\xi}, \spr{\cH}/4}$.
    %and a test $t$ covers $h\in \cH$ if $\brc{U_{t}\br{u}}\leq \frac{3}{4}\cdot \brc{\mathcal{U}}$.
    Then, $\spr{S^*} \leq \spr{\sepcover{\cI}}$.
\end{lemma}
    \begin{proof}
        It is enough to argue that $\xi(\sepcover{\cI})$ covers at least $\spr{\cH/4}$ elements from $\cH$.
        Assume towards a contradiction that this is not the case, i. e. less than $\spr{\cH}/4$ elements are covered by $\xi(\sepcover{\cI})$.
%Therefore there exists $t\in \sepcover{\cI}$ and a response $\cH'$ of size $\spr{\cH'}\leq \spr{\cH}/2$ such that hypotheses in $\cH'$ are not covered by $\sepcover{\cI}$, otherwise the claim holds trivially, since all hypotheses are covered. Let $\cH'\subseteq U_{t,j}$ (since $\cH'$ is a response to a test $t$, such $U_{t,j}$ always exists). By assumption, we have that $\spr{U_{t,j}-\cH'} < \spr{\cH}/4$. Therefore, we have that $\spr{U_{t,j}} = \spr{\cH'}+\spr{U_{t,j}-\cH'}<3/4\cdot \spr{\cH}$ which by definition means that $h$ is covered by $\sepcover{\cI}$, a contradiction.
        Therefore there exists $t\in \sepcover{\cI}$ and a response $U\in t$ that corresponds to a child $u$ of $t$ such that hypotheses in $U$ are not covered by $\xi(\sepcover{\cI})$ and $\spr{\cH_u}\leq\spr{\cH}/2$.
        (Here we refer to a decision tree $D$ such that $\sepcover{D}=\sepcover{\cI}$).
        Otherwise the claim holds trivially, since all hypotheses would be covered.
        Note that $\cH_u\subseteq U$.
        We have that $U\setminus\cH_u$ is a subset of hypotheses covered by other tests in $\sepcover{\cI}$ and hence by assumption, $\spr{U\setminus\cH_u}\leq \spr{\cH}/4$.
        Therefore, we have that $\spr{U} = \spr{\cH_u}+\spr{U\setminus\cH_u}<3/4\cdot \spr{\cH}$ which by definition means that all elements in $U$ are covered by $\sepcover{\cI}$, a contradiction.
    \end{proof}


%\DD{I tak bedziemy oszczedzac miejsce, wiec moze nie warto parafrazowac algorytmu, a dac jakas intuicje jako klej?}
%We now state an algorithm (Algorithm~\ref{alg:worstDecisionTree}) that outputs a decision tree for an input instance $\cI=\br{\cH, \cT, \cF}$ of $\ProblemPCWCAL$.

%     Algorithm \ref{alg:worstDecisionTree} is recursive and works as follows.
%     Given an instance $\cI=\br{\cH, \cT, \cF}$, if $\spr{\cH}=1$ we return the trivial decision tree with a single leaf corresponding to the only hypothesis in $\cH$. Otherwise, we run the $\br{\gamma,\alpha}$-approximation algorithm for $\ProblemPCSC$ on instance $\br{\cH, \xi(\cT), \preceq_{\xi}, \spr{\cH}/4}$.
%     Let $S\subseteq\cT$ be the output. We build a decision tree $D_S$ on tests from $S$ closed under $\preceq_{\xi}$.
%     For each $\cH' \in \cH\setminus S$, we recursively call \ProcWorstDecisionTree on instance $\br{\cH', \cT\setminus S, \cF-S}$ and attach the returned decision tree to the leaf of $D_S$ corresponding to $\cH'$. Finally, we return the constructed decision tree $D$.


The following observation is due to Lemmas \ref{lemma:subspace_opt} and \ref{lemma:cover_sep_worst_case}.:
    \begin{observation} \label{obs:DS}
        Consider $D_S$ computed in Algorithm~\ref{alg:worstDecisionTree}, that uses a $\br{\gamma,\alpha}$-approximation algorithm for $\ProblemPCSC$ as the $\blackBox$ subroutine.
        %Let $D_S$ be the decision tree built on tests from $S$ respecting the precedence constraints $\cF$.
        Then, $\COSTW\br{D_S} \leq \alpha \cdot \spr{\sepcover{\cI}}$.
    \end{observation}


\begin{theorem} \label{thm:alphaPCWCAL}
If there is an $\br{\gamma, \alpha}$-bicriteria approximation algorithm for $\ProblemPCSC$, then there is an
$O\br{\frac{\alpha}{\log\br{\frac{2\gamma}{2\gamma -1}}} \cdot \log n}$-approximation algorithm for $\ProblemPCWCAL$. In particular when $\gamma = O\br{1}$, the approximation is $O\br{\alpha \cdot \log n}$.
\end{theorem}
\begin{proof}
     Consider a call to \ProcWorstDecisionTree on input $\cI=\br{\cH, \cT, \cF, \blackBox}$, where $\blackBox$ is a $\br{\gamma,\alpha}$-approximation algorithm for $\ProblemPCSC$.
     Let $D$ be the decision tree returned by this call.
     In order to prove the theorem it is enough to argue that
     \begin{equation} \label{eq:D}
     \COSTW\br{D} \leq \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n \cdot \OPTW\br{\cI}.
     \end{equation}

     The proof is by induction on $n$. The base case when $n=1$ is trivial since the the cost of the decision tree is 0.
     Assume by induction that for every $\cI' = \br{\cH', \cT, \cF}$ such that $\cH' \subseteq \cH\setminus S$ and $n' = \spr{\cH'}$ we have $\COSTW\br{D'} \leq \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n' \cdot \OPTW\br{\cI'}$, where $D'$ is the decision tree returned by \ProcWorstDecisionTree on input $\cI'$.
     To point out the dependency of $D'$ on $\cH'$, we write $D'(\cH')$.
     We have
%         \begin{align*}
%             \COSTW\br{D, \cI} \leq & \COSTW\br{D_S, \cI} + \max_{\cH' \in \cH \setminus S} \COSTW\br{D', \cI'} \\
%             \leq & \alpha \cdot \spr{S^*} + \max_{\cH' \in \cH \setminus S} \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n' \cdot \OPTW\br{\cI'} \\
%             \leq & \alpha \cdot \spr{\sepcover{\cI}} + \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log \br{\frac{\br{{4\gamma-1}}\cdot n}{4\gamma}} \cdot \OPTW\br{\cI} \\
%             = & \alpha \cdot \OPTW\br{\cI} + \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n \cdot \OPTW\br{\cI} - \alpha\cdot \OPTW\br{\cI} \\
%             = & \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n \cdot \OPTW\br{\cI} \\
%        \end{align*}
        \begin{align*}
            \COSTW\br{D} \leq & \COSTW\br{D_S} + \max_{\cH' \in \cH \setminus S} \COSTW\br{D'(\cH')} \\
            \leq & \alpha \cdot \spr{\sepcover{\cI}} + \max_{\cH' \in \cH \setminus S} \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n' \cdot \OPTW\br{\cI'} \\
            \leq & \alpha \cdot \OPTW\br{\cI} + \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log \br{\frac{\br{{4\gamma-1}}\cdot n}{4\gamma}} \cdot \OPTW\br{\cI} \\
            = & \alpha \cdot \OPTW\br{\cI} + \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n \cdot \OPTW\br{\cI} - \alpha\cdot \OPTW\br{\cI} \\
            = & \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n \cdot \OPTW\br{\cI},
        \end{align*}
        where the second inequality follows by the induction hypothesis and Observation~\ref{obs:DS}, the third inequality follows by Observation~\ref{obs:sepcoverWC}, Lemma \ref{lemma:subspace_opt} and the fact that $n'=\spr{\cH'} \leq \frac{(4\gamma -1)}{4\gamma}\cdot n$ for every $\cH' \in \cH\setminus S$.
        This concludes the proof of the theorem.
\end{proof}


\subsection{Precedence Constrained Average Case Active Learning} \label{subsection:PCACAL}

We follow a similar idea, however we use the connection to $\ProblemPCMSSC$ instead of $\ProblemPCSC$.
This allows to use $\COSTA\br{\sepcover{\cI}}$ as a lower bound on $\OPTA\br{\cI}$ in the analysis of the approximation algorithm for $\ProblemPCACAL$.
\DD{do rozwazenia fix notacyjny: $\sepcover{\cI}$ to jest zbior, a tutaj mamy do czynienia z suma zalezna od kolejnosci}
However, since in this case we have arbitrary probability distrubution on hypotheses, we firstly emply a rounding technique which firstly scales the probabilities to polynomial size integers. Below we present which does so, by incurring only a 2-factor loss in the approximation.
\begin{lemma} \label{lemma:rounding}
    Let $\cI=\br{\cH, \cT, \cF, p}$ be any instance of $\ProblemPCACAL$ with $n=\spr{\cH}$.
    Then, there exists an instance $\cI'=\br{\cH, \cT, \cF, p'}$ such that for each $h\in\cH$, $p'(h)\in \mathbb{N}$ and $p\br{\cH}=O(nm)$ and $\OPTA\br{\cI'} \leq 2 \cdot \OPTA\br{\cI}$.
\end{lemma}
\begin{proof}
    Let $K=\frac{1}{nm}$. For every $h\in\cH$, let $p'(h) = \cl{p(h)/K}$. Therefore, $p'(h) \leq p(h)/K + 1$ and as a consequence, $p'\br{\cH} \leq \sum_{h \in \cH} \br {\frac{p(h)}{K} + 1} = \frac{1}{K} \sum_{h \in \cH} p(h) + n = nm + n = O(nm)$. 

    We now argue that $\OPTA\br{\cI'} \leq 2 \cdot \OPTA\br{\cI}$. Let $D'$ be an optimal decision tree for $\cI'$ and let $D$ be the optimal decision tree for $\cI$.
    \begin{align*}
    \COSTA\br{D', \cI} &\leq K\cdot \COSTA\br{D', \cI'}\\&\leq 
    K\cdot \COSTA\br{D, \cI'}
    \\&\leq
    \COSTA\br{D, \cI} + K\cdot\sum_{h\in\cH} \COST\br{D, \cI', h}
    \\&\leq
    \COSTA\br{D, \cI} + K\cdot n\cdot m
    \\&=
    \COSTA\br{D, \cI} + 1
    \\&\leq
    2 \cdot \COSTA\br{D, \cI}
    \end{align*}
    where the first inequality follows since clearly, for every $h\in\cH$, we have that $p(u)\leq K\cdot p'(h)$, the second inequality is due to the optimality of $D$ for $\cI$, the third inequality follows since for every $h\in\cH$, $K\cdot p'(h) \leq p(h) + K$, the fourth inequality follows since the depth of $D$ is at most $m$, the equality is by definition of $K$ and the last inequality follows since $\COSTA\br{D, \cI} \geq p\br{\cH} = 1$.
\end{proof}

Therefore, from now on we will assume that the input instance $\cI=\br{\cH, \cT, \cF, p}$ of $\ProblemPCACAL$ is such that for each $h\in\cH$, $p(h)\in \mathbb{N}$ and $p\br{\cH}=O(nm)$, without impacting the asymptotic approximation ratio of our algorithm.
We have the following lemma, analogous to Lemma \ref{lemma:cover_sep_worst_case}.
\begin{lemma}\label{lemma:cover_sep_average_case}
    Let $\cI=\br{\cH, \cT, \cF}$ be any $\ProblemPCACAL$ instance. Let $S^*$ be an optimal solution to $\ProblemPCMSSC$ on instance $\br{\cH, \xi(\cT), \preceq_{\xi}, f}$ with $f=1/4$.
    %, where a test $t$ covers element $u \in \cH$ if for $u\in U_{t,j}$, $\spr{U_{t,j}}\leq \frac{3}{4}\cdot \spr{\cH}$.
    Then, $\coverageTime\br{S^*} \leq \COSTA\br{\sepcover{\cI}}$.
\end{lemma}
\begin{proof}
    Assume towards a contradiction that $\coverageTime\br{S^*} > \COSTA\br{\sepcover{\cI}}$.
    We will show that in such case there exists a cover $\sigma \subseteq \xi(\sepcover{\cI})$ such that $\coverageTime\br{\sigma} < \COSTA\br{\sepcover{\cI}}$, contradicting the optimality of $S^*$.
    Let $\sigma$ be the shortest prefix of $\sepcover{\cI}$ that covers at least $\spr{\cH}/4$ elements, $\spr{\bigcup\br{\sigma}}\geq \spr{\cH}/4$.
    Note that because we consider a prefix, the precedence constraints are satisfied.
    Such a subsequence exists, by repeating the argument used in the proof of Lemma \ref{lemma:cover_sep_worst_case}. \DD{W sumie moze sie powolac na lemma \ref{lemma:cover_sep_worst_case} bo ma w sformulowaniu ten bound (?)}
    Take an arbitrary $h \in \cH$.
    There are two cases to consider:
    \begin{itemize}
        \item $\sigma$ covers $h$. Consider the first test $t$ that sepcovered $h$ in $\sepcover{\cI}$. By definition, tests previous to $t$ in $\sigma$ cover at most $\spr{\cH}/4$ elements. Since at the moment of sepcovering, $h$ belonged to a response of size at most $\spr{\cH}/2$, we know that $t$ also covers $h$ in $\sigma$. This means that the contribution of $h$ to $\coverageTime\br{\sigma}$ is at most its contribution to $\COSTA\br{\sepcover{\cI}}$.
        \item $\sigma$ does not cover $h$. Since $h$ is sepcovered by some test $t$ in $\sepcover{\cI}$ but not covered by $\sigma$, the contribution of $h$ to $\coverageTime\br{\sigma}$ is $\spr{\sigma}$ and its contribution to $\COSTA\br{\sepcover{\cI}}$ is at least $\spr{\sigma}$.
        \DD{fix w def. kryterium: niepokryte elementy kontrybuuja dlugosc sekwencji}
    \end{itemize}
    Thus, we have that $\coverageTime\br{\sigma} < \COSTA\br{\sepcover{\cI}}$, a contradiction.
\end{proof}
    
\begin{theorem}
    If there is a $\beta$-approximation algorithm for $\ProblemPCMSSC$, then there is a $O\br{\beta \cdot \log n}$-approximation algorithm for $\ProblemPCACAL$.
\end{theorem}
\begin{proof}
    Consider a call to \ProcWorstDecisionTree on input $\cI=\br{\cH, \cT, \cF, \blackBox}$, where $\blackBox$ is a $\beta$-approximation algorithm for $\ProblemPCMSSC$ for the instance $(\cH, \xi(\cT), \preceq_{\xi}, \spr{\cH}/4)$.
    Let $D$ be the output and let $D_S$ be the decision tree consisting of the sequence of tests $S$, where $S$ is the result of the call to the $\blackBox$.
    Then we have
    \begin{equation} \label{eq:DSaverage}
    \COSTA\br{D_S} \leq \beta \cdot \COSTA\br{\sepcover{\cI}}
    \end{equation}
% The idea behind Algorithm \ref{alg:averageDecisionTree} is the same as for the worst case version of the problem except the fact that we use a solution to $\ProblemPCMSSC$ instead of $\ProblemPCSC$.
% \input{pseudocodes/average_case_learning.tex}

    We prove by induction on $n$ that
    \begin{equation} \label{eq:ACinduction}
     \COSTA\br{D} \leq \beta \cdot \log_{4/3} n \cdot \OPTA\br{\cI}.
    \end{equation}

    The base case when $n=1$ is trivial since the the cost of the decision tree is 0.
    Assume by induction that \eqref{eq:ACinduction} holds for smaller $n$, in particular for every $\cI' = \br{\cH', \xi(\cT), \preceq_{\xi}}$ such that $\cH' \in \cH\setminus S$ and $n' = \spr{\cH'}$ we have $\COSTA\br{D'} \leq \beta \cdot \log_{4/3} n' \cdot \OPTA\br{\cI'}$, where $D'$ is any of the decision trees returned by the recursive calls to \ProcWorstDecisionTree on input $\cI'$. We have that:
\begin{align*}
    \COSTA\br{D} \leq & \COSTA\br{D_S} + \sum_{\cH' \in \cH \setminus S} \COSTA\br{D'} \\
    \leq & \beta \cdot \COSTA\br{\sepcover{\cI}} + \sum_{\cH' \in \cH \setminus S} \beta \cdot \log_{4/3} n' \cdot \OPTA\br{\cI'} \\
    \leq & \beta \cdot \COSTA\br{\sepcover{\cI}} + \sum_{\cH' \in \cH \setminus S} \beta \cdot \log_{4/3} \br{\frac{3}{4}\cdot n} \cdot \OPTA\br{\cI} \\
    = & \beta \cdot \COSTA\br{\sepcover{\cI}} + \beta\cdot \br{\log_{4/3} n - 1} \cdot \OPTA\br{\cI} \\
    \leq & \beta\cdot \log_{4/3} n \cdot \OPTA\br{\cI},
\end{align*}
where the first inequality is due to Observation~\ref{obs:WCdecomposition}, the second inequality follows from \eqref{eq:ACinduction} and Observation~\ref{obs:sepcoverAC}, and the third inequality follows from Lemma \ref{lemma:subspace_opt} and the fact that $\spr{\cH'} \leq \frac{3}{4}\cdot n$ for every $\cH' \in \cH\setminus S$.
This concludes the proof.
\end{proof}
