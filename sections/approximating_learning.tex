\section{Active Learning via Covering Problems}

We begin with the following folklore lemma concerning both worst and average case learning.
\begin{lemma}\label{lemma:subspace_opt}
    Let $I=\br{\mathcal{H}, \mathcal{T},\mathcal{F}}$ be any PCAL instance. Let $\mathcal{H}'\subseteq \mathcal{H}$. Then $\OPT\br{\mathcal{H}', \mathcal{T}, \mathcal{F}} \leq \OPT\br{I}$.
\end{lemma}
\subsection{Worst Case}

\begin{definition}[Coversep]
    Let $D$ be any decision tree for $\mathcal{I}=\br{\mathcal{H}, \mathcal{T}, \mathcal{F}}$. We define a sequence of tests $P_D$ called \emph{coversep} as follows. Initially, $P_D$ is empty and $\mathcal{H}' = \mathcal{H}$. While $\spr{\mathcal{H}'}> \spr{\mathcal{H}}/2$, we append to $P_D$ the test $r\br{D_{\mathcal{H}'}}$ and update $\mathcal{H}'$ to be the set of hypotheses corresponding to the child of $D_{\mathcal{H}'}$ that contains the most hypotheses. If $\COST\br{D}=\OPT\br{\mathcal{I}}$, then we denote $P^*\br{\mathcal{I}} = P_D$ (ties broken arbitrarily).
\end{definition}

It should be remarked that $P_D$ is well-defined, as each test in $P_D$ can have at most one child associated with more than half of the hypotheses in $\mathcal{H}'$. Since $P_D$ is a subpath of $D$, we also have the following simple observation.

\begin{observation}
    Let $I$ be any instance of PCWCAL. Then $\spr{P^*\br{I}} \leq \OPT\br{I}$.
\end{observation}

We will use $\spr{P^*\br{I}}$ as a lower bound on $\OPT\br{I}$ in the analysis of the approximation algorithm for PCWCAL. We have the following lemma:
\begin{lemma}\label{lemma:pairsep_cost}
    Let $I=\br{\mathcal{H}, \mathcal{T}, \mathcal{F}}$ be any PCWCAL instance. Let $S^*$ be the optimal solution for the PCSC on instance $\br{\mathcal{H}, \mathcal{T}, \mathcal{F}, f}$ where $f=1/4$ and a test $t$ covers $h\in \mathcal{H}$ if $\brc{U_{t}\br{u}}\leq \frac{3}{4}\cdot \brc{\mathcal{U}}$. Then, $\spr{S^*} \leq \spr{P^*\br{I}}$.
\end{lemma}
    \begin{proof}
        We show that $P^*\br{I}$ is a feasible solution for the PCSC instance $\br{\mathcal{H}, \mathcal{T}, \mathcal{F},K}$. Assume towards a contradiction that this is not the case, i. e. less than $\spr{\mathcal{H}}/4$ are covered by $P^*\br{I}$. Therefore there exists $t\in P^*\br{I}$ and a response $\mathcal{H}'$ of size $\spr{\mathcal{H}'}\leq \spr{\mathcal{H}}/2$ such that hypotheses in $\mathcal{H}'$ are not covered by $P^*\br{I}$, otherwise the claim holds trivially, since all hypotheses are covered. Let $\mathcal{H}'\subseteq U_{t,j}$ (since $\mathcal{H}'$ is a response to a test $t$, such $U_{t,j}$ always exists). By assumption, we have that $\spr{U_{t,j}-\mathcal{H}'} < \spr{\mathcal{H}}/4$. Therefore, we have that $\spr{U_{t,j}} = \spr{\mathcal{H}'}+\spr{U_{t,j}-\mathcal{H}'}<3/4\cdot \spr{\mathcal{H}}$ which by definition means that $h$ is covered by $P^*\br{I}$, a contradiction.
    \end{proof}

\begin{theorem}
    If there is an $\br{\gamma, \alpha}$-bicriteria approximation algorithm for PCSC then there is an
$O\br{\frac{\alpha}{\log\br{\frac{2\gamma}{2\gamma -1}}} \cdot \log n}$-approximation algorithm for PCWCAL. In particular when $\gamma = O\br{1}$, the approximation is $O\br{\alpha \cdot \log n}$.
\end{theorem}
\begin{proof}
\input{pseudocodes/worst_case_learning.tex}
The following observation follows by Lemmas \ref{lemma:subspace_opt} and \ref{lemma:pairsep_cost}:
    \begin{observation}
        Let $D_S$ be the decision tree built on tests from $S$ closed under $\mathcal{F}$. Then, $\COST\br{D_S} \leq \alpha \cdot \spr{P^*\br{I}}$.
    \end{observation}
    We are now ready to prove the theorem.
    \begin{lemma}
        Let $D$ be the decision tree returned by \textsc{WorstDecisionTree} on input $I=\br{\mathcal{H}, \mathcal{T}, \mathcal{F}}$. Then, $\COST\br{D} \leq \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n \cdot \OPT\br{I}$.
    \end{lemma}
    \begin{proof}
        We prove the lemma by induction on $n$. The base case when $n=1$ is trivial since the the cost of the decision tree is 0. Assume by induction that for every $I' = \br{\mathcal{H}', \mathcal{T}, \mathcal{F}}$ such that $\mathcal{H}' \in \mathcal{H}-S$ and $n' = \spr{\mathcal{H}'}$ we have $\COST\br{D'} \leq \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n' \cdot \OPT\br{I'}$, where $D'$ is the decision tree returned by \textsc{WorstDecisionTree} on input $I'$. We have that:
        $$
        \begin{align*}
            \COST\br{D} \leq & \COST\br{D_S} + \max_{\mathcal{H}' \in \mathcal{H} - S} \COST\br{D'} \\
            \leq & \alpha \cdot \spr{S^*} + \max_{\mathcal{H}' \in \mathcal{H} - S} \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n' \cdot \OPT\br{I'} \\
            \leq & \alpha \cdot \spr{P^*\br{I}} + \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log \br{\frac{\br{{4\gamma-1}}\cdot n}{4\gamma}} \cdot \OPT\br{I} \\
            = & \alpha \cdot \OPT\br{I} + \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n \cdot \OPT\br{I} - \alpha\cdot \OPT\br{I} \\
            = & \frac{\alpha}{\log\br{\frac{4\gamma}{4\gamma -1}}}\cdot \log n \cdot \OPT\br{I} \\
        \end{align*}
        $$
        
        where the second inequality follows by the induction hypothesis, the third inequality follows by Lemma \ref{lemma:subspace_opt} and the fact that $\spr{\mathcal{H}'} \leq \frac{(4\gamma -1)}{4\gamma}\cdot n$ for every $\mathcal{H}' \in \mathcal{H}-S$ and the equalities follow by rearranging terms. This concludes the proof of the lemma.
    \end{proof}
\end{proof}
\subsection{Average Case}
It should be remarked that $C_D$ is well-defined, as each test in $C_D$ can have at most one child associated with more than half of the hypotheses in $\mathcal{H}'$. Since $C_D$ is a subpath of $D$, we also have the following simple observation.

\begin{observation}
    Let $I$ be any instance of PCACAL. Then $\COST\br{C^*\br{I}} \leq \OPT\br{I}$.
\end{observation}

This allows to use $\COST\br{C^*\br{I}}$ as a lower bound on $\OPT\br{I}$ in the analysis of the approximation algorithm for PCACAL. We have the following lemma:
\begin{lemma}\label{lemma:pairsep_cost}
    Let $I=\br{\mathcal{H}, \mathcal{T}, \mathcal{F}}$ be any PCACAL instance. Let $S^*$ be the optimal solution for the PCMSSC on instance $\br{\mathcal{U}, \mathcal{T}, \mathcal{F}, f}$ with $f=1/4$, where $\mathcal{U} = \mathcal{H}$ and a test $t$ element $u \in \mathcal{U}$ if for $u\in U_{t,j}$, $\spr{U_{t,j}}\leq \frac{3}{4}\cdot \spr{\mathcal{U}}$. Then, $\COST\br{S^*} \leq \COST\br{C^*\br{I}}$.
\end{lemma}
    % \begin{proof}
    %     We will treat the sequence $C^*\br{I}$ as a path in a decision tree. We show, that if a given response to a query in this path is associated with a subtree which does not contain queries from $C^*\br{I}$, then the amount of hypotheses is such response is at most $\frac{3}{4}\cdot \spr{\mathcal{H}}$. 
    %     $$
    %     \binom{\spr{\mathcal{H}}}{2} - \sum_{\mathcal{H}' \in \mathcal{H} - P^*\br{I}}\binom{\spr{\mathcal{H}'}}{2} \geq \binom{\spr{\mathcal{H}}}{2} - \frac{\binom{\spr{\mathcal{H}}}{2}}{2} = \frac{\binom{\spr{\mathcal{H}}}{2}}{2}.
    %     $$
    %     Therefore, by the optimality of $S^*$, we have $\spr{S^*} \leq \spr{P^*\br{I}}$ as required.
    % \end{proof}
\begin{theorem}
    If there is a $\beta$- approximation algorithm for PCMSSC then there is an
$O\br{\beta \cdot \log n}$-approximation algorithm for PCACAL.
\end{theorem}
\input{pseudocodes/average_case_learning.tex}
