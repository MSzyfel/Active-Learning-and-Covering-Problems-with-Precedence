\section{Preliminaries}
\begin{definition}[Precedence constrained set cover (PCSC)]
Given a universe $\mathcal{U}$ of $n$ items, a collection $\mathcal{S}$ of $m$ subsets of $\mathcal{U}$, a DAG $\mathcal{F} = \brc{\mathcal{S}, \preceq}$ encoding precedence constraints, and a coverage requirement $K$, find a precedence-closed subfamily $\mathcal{C} \subseteq \mathcal{S}$ that covers at least $K$ items while minimizing $\spr{\mathcal{C}}$.
\end{definition}
\begin{definition}[Precedence constrained min-sum set cover (PCMSSC)]
Given a universe $\mathcal{U}$ of $n$ items, a collection $\mathcal{S}$ of $m$ subsets of $\mathcal{U}$, a DAG $\mathcal{F} = \brc{\mathcal{S}, \preceq}$ encoding precedence constraints, and a coverage requirement $K$, find a precedence-closed sequence of sets that covers at least $K$ items while minimizing the average time (position in sequence) at which items are covered.
\end{definition}
\begin{definition}[Precedence constrained test cover (PCTC)]
Given a set $\mathcal{H}$ of $n$ hypotheses, a set $\mathcal{T}$ of $m$ tests, and a DAG $\mathcal{F} = \brc{\mathcal{T}, \preceq}$ encoding precedence constraints, find a precedence-closed subfamily of tests that distinguishes all pairs of hypotheses.
\end{definition}

In the active learning problems, we are given a set of hypotheses $\mathcal{H}$ and a set of tests $\mathcal{T}$. Each test $t$ is an arbitrary partition of $\mathcal{H}$ into disjoint subsets $U_{t,1}, U_{t,2}, \ldots, U_{t,r_t}$, where $r_t$ is the number of possible responses to test $t$. When a test $t$ is performed, the response indicates which subset $U_{t,j}$ contains the hidden hypothesis $\target \in \mathcal{H}$. The tests are subject to precedence constraints encoded by a DAG $\mathcal{F} = \brc{\mathcal{T}, \preceq}$, meaning that a test $t$ can be performed only if all its predecessors in $\mathcal{F}$ have already been performed.

The goal is to design a \textit{strategy} of learning which is an adaptive algorithm, that based on the previous responses provides the searcher with the next test to be performed. We represent this strategy as a rooted tree called a \textit{decision tree} $D$, where each internal node represents a test to be performed and each leaf represents a hypothesis. We demand that each edge outgoing from the root $r$ of $D$ is associated with a unique response to a test $r$ and that the same holds for all decision subtrees of $D-r$ (which are not leafs). It should be remarked that it is possible for a test to appear multiple times in the decision tree. Therefore, by $\mathcal{T}_D$ we denote subset of $\mathcal{T}$ that appear in decision tree $D$ and by $T_D$ we denote the set of inner nodes of $D$, so that each usage of a test in $D$ corresponds to a unique element of $T_D$.

Let $T_{D, h}$ denote the sequence of tests performed when the hidden hypothesis is $h$ and the learner follows the strategy represented by decision tree $D$. The cost of identifying hypothesis $h$ using decision tree $D$ is defined as $\COST\br{D, h} = \spr{T_{D, h}}$. We consider two cost measures for decision trees: the worst-case cost $\COSTW\br{D} = \max_{h \in \mathcal{H}} \COST\br{D, h}$ and the average-case cost $\COSTA\br{D} = \sum_{h \in \mathcal{H}} \COST\br{D, h}$ (up to a multiplicative factor of $1/\spr{\mathcal{H}}$).

\begin{definition}[Precedence constrained worst case active learning (PCWCAL)]
Given a set $\mathcal{H}$ of $n$ hypotheses, a set $\mathcal{T}$ of $m$ tests, and a DAG $\mathcal{F} = \brc{\mathcal{T}, \preceq}$ encoding precedence constraints, construct a decision tree respecting precedence constraints that identifies any hypothesis from $\mathcal{H}$ while minimizing the worst-case depth of the tree.
\end{definition}
\begin{definition}[Precedence constrained average case active learning (PCACAL)]
Given a set $\mathcal{H}$ of $n$ hypotheses, a set $\mathcal{T}$ of $m$ tests and a DAG $\mathcal{F} = \brc{\mathcal{T}, \preceq}$ encoding precedence constraints, 
% and a probability distribution over $\mathcal{H}$, 
construct a decision tree respecting precedence constraints that identifies any hypothesis from $\mathcal{H}$ while minimizing the expected depth (average case cost).
\end{definition}

Let $\mathcal{H}_t$ denote the set of hypotheses that are not yet distinguished by the tests selected before test $t\in T_D$ in the decision tree. Then we immediately obtain the following simple observation:
\begin{observation}
    Let $D$ be any decision tree for $\mathcal{I}=\br{\mathcal{H}, \mathcal{T}, \mathcal{F}}$. Then we have that:
    $$
    \COSTA\br{D} = \sum_{t \in T_D} \spr{\mathcal{H}_t}
    $$
\end{observation}

We will also make use of the following folklore lemmas which are due to the fact that any decision tree for $I$ can be restricted to a decision tree for each subproblem $\br{\mathcal{H}_i, \mathcal{T}, \mathcal{F}}$ without increasing the cost:

\begin{lemma}\label{lemma:subspace_opt}
    Let $I=\br{\mathcal{H}, \mathcal{T},\mathcal{F}}$ be any PCAL instance. Let $\mathcal{H}'\subseteq \mathcal{H}$. Then $\OPT\br{\mathcal{H}', \mathcal{T}, \mathcal{F}} \leq \OPT\br{I}$.
\end{lemma}
\begin{lemma}\label{lemma:subinstances_avg}
    Let $I=(\mathcal{H}, \mathcal{T}, \mathcal{F})$ be an instance of $PCACAL$. Let $\mathcal{H}_1,\dots,\mathcal{H}_t\subseteq \mathcal{H}$ such that $\bigcup_{i=1}^t \mathcal{H}_i \subseteq \mathcal{H}$ and for any $i\neq j$, $\mathcal{H}_i\cap \mathcal{H}_j = \emptyset$. Then we have that:
    $$
    \OPTA\br{I} \geq \sum_{i=1}^t \OPTA\br{\br{\mathcal{H}_i, \mathcal{T}, \mathcal{F}}}
    $$
\end{lemma}



\begin{definition}[Group Steiner Tree (GST)]
Given an undirected graph $G = (V, E)$ with edge costs, a root vertex $r \in V$, and groups $g_1, \ldots, g_k \subseteq V$, find a minimum-cost tree $T$ rooted at $r$ that contains at least one vertex from each group $g_i$.
\end{definition}

For a subfamily $\mathcal{A} \subseteq \mathcal{F}$, we define the coverage as:
\[
\cov\br{\mathcal{A}} \equiv \bigcup_{A \in \mathcal{A}} A
\]
For a subset $X$ of the universe, the coverage on $X$ is:
\[
\cov\br{\mathcal{A}, X} = \cov\br{\mathcal{A}} \cap X
\]

The density $\Delta$ of a nonempty subfamily $\mathcal{A}$ on subset $X$ is:
\[
\Delta\br{\mathcal{A}, X} \equiv \frac{\spr{\cov\br{\mathcal{A}, X}}}{\spr{\mathcal{A}}}
\]
For convenience, we define $\Delta\br{\emptyset, X} < 0$.

\begin{definition}[Max-Density Precedence-Closed Subfamily (MDPCS)]
Given a family of $m$ sets $\mathcal{G}$, a precedence relation $\prec$, and a set of $n$ items to be covered $R \subseteq \cov\br{\mathcal{G}}$, the MDPCS problem asks to find a precedence-closed subfamily $\mathcal{A} \subseteq \mathcal{G}$ that maximizes $\Delta\br{\mathcal{A}, R}$.
\end{definition}


For $S \in \mathcal{G}$, let $P[S]$ denote the minimal precedence-closed subfamily of $\mathcal{G}$ containing $S$ (i.e., the ancestors of $S$ including $S$ itself).

