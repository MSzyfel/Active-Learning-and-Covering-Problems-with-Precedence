\section{Preliminaries}
% \begin{definition}[Precedence constrained set cover (PCSC)]
% Given a universe $\mathcal{U}$ of $n$ items, a collection $\mathcal{S}$ of $m$ subsets of $\mathcal{U}$, a DAG $\cF = \brc{\mathcal{S}, \preceq}$ encoding precedence constraints, and a coverage requirement $K$, find a precedence-closed subfamily $\mathcal{C} \subseteq \mathcal{S}$ that covers at least $K$ items while minimizing $\spr{\mathcal{C}}$.
% \end{definition}
% \begin{definition}[Precedence constrained min-sum set cover (PCMSSC)]
% Given a universe $\mathcal{U}$ of $n$ items, a collection $\mathcal{S}$ of $m$ subsets of $\mathcal{U}$, a DAG $\cF = \brc{\mathcal{S}, \preceq}$ encoding precedence constraints, and a coverage requirement $K$, find a precedence-closed sequence of sets that covers at least $K$ items while minimizing the average time (position in sequence) at which items are covered.
% \end{definition}

% In the active learning problems, we are given a set of hypotheses $\cH$ and a set of tests $\cT$. Each test $t$ is an arbitrary partition of $\cH$ into disjoint subsets $U_{t,1}, U_{t,2}, \ldots, U_{t,r_t}$, where $r_t$ is the number of possible responses to test $t$. When a test $t$ is performed, the response indicates which subset $U_{t,j}$ contains the hidden hypothesis $\target \in \cH$. The tests are subject to precedence constraints encoded by a DAG $\cF = \brc{\cT, \preceq}$, meaning that a test $t$ can be performed only if all its predecessors in $\cF$ have already been performed.

\paraTitle{Decision trees.}
In all adaptive learning problems, the goal is to design a \textit{strategy} $\cS$ understood as an adaptive algorithm, that based on the previous responses provides the \questioner with the next test to be performed.
We represent this strategy as a rooted tree $D$ called a \textit{decision tree}, where each internal node represents a test to be performed and each leaf represents a hypothesis.
Formally, a decision tree is defined recursively as follows.
If $r=\br{R_1,\ldots,R_l}\in\cT$ is the first test performed by $\cS$, then $r$ is the root of $D$.
For each reply $R_i$, $i\in\brc{1,\ldots,l}$, take inductively defined decision tree $D_i$ derived from the queries done by $\cS$ when the reply to $r$ is $R_i$.
Then, $D$ is obtained by making the roots of $D_1,\ldots,D_l$ the children of $r$.
To complete the inductive definition, whenever $\cS$ finalizes the search by declaring that $h\in\cH$ is the target, the corresponding decistion subtree is the vertex $h$.
Hence, each edge outgoing from the root $r$ of $D$ is associated with a unique response to a test $r$, and the same holds for all internal
It should be remarked that it is possible for a test to appear multiple times in the decision tree.
Hence, with a slight abuse of notation we use tests as vertices of $D$ but whenever a given test provides many vertices in $D$ we make clear which one we refer to.

For any node $v$ of $D$, $D_v$ is the subtree of $D$ that consists of $v$ and all its descendants.
We write $\inner{D}$ we denote the set of nodes in $D$ that are not leaves.
Hence, each usage of a test in $D$ corresponds to a unique element of $\inner{D}$.

\medskip
\paraTitle{Cost measures.}
Let $\tests{D}{v}$, denote the sequence of tests performed in order to arrive at the node $v$ in $D$, including $v$.
In other words, $\tests{D}{v}$ consists of the nodes of the path from the root to $v$ in $D$.
The cost of identifying hypothesis $h$ using a decision tree $D$ is $\COST\br{D, h}=\spr{\tests{D}{v}}$, where $v$ is the leaf of $D$ that represent the event of declaring that $h$ is the target.
We consider two cost measures for decision trees: the worst-case cost $\COSTW\br{D} = \max_{h \in \cH} \COST\br{D, h}$ and the average-case cost $\COSTA\br{D} = \sum_{h \in \cH} \COST\br{D, h}$ (up to a multiplicative factor of $1/\spr{\cH}$).
The criteria we consider for any $\ProblemPCAL$ instance $I$ are denoted by
$$\OPTA\br{\cI}=\min\brc{\COSTA\br{D} \mid D\textup{ is a decision tree}},$$
$$\OPTW\br{\cI}=\min\brc{\COSTW\br{D} \mid D\textup{ is a decision tree}}.$$
\DD{Skoro mamy ju≈º te problemy $\ProblemPCWCAL$ oraz $\ProblemPCACAL$ zdefiniowane, to tutaj aby nie powtarzac w nowej notacji drze decyzyjnych zamienie to na uwage/obserwacje w ramach jednego paragrafu do optymalizacji czego to sie sprowadza odnosnie drzew dec.}
Hence the $\ProblemPCACAL$ aims at finding a decision tree $D$ such that $\COSTA\br{D}=\OPTA\br{\cI}$ and in case of $\ProblemPCWCAL$ the goal is to find $D$ such that $\COSTW\br{D}=\OPTW\br{\cI}$.


For a sequence $S$ being a solution to $\ProblemPCMSSC$, we use the symbol $\coverageTime\br{S}$ to denote the sum of coverage times of all elements in $\bigcup S$ (in case of dealing with sequences, with a slight abuse of notation we use the set notaion).
Then, for an instance $\cI=\br{\cU,\cS,\preceq,k}$ of $\ProblemPCMSSC$, $\OPTA\br{\cI}=\min\brc{\coverageTime\br{S} \mid \spr{\bigcup S}\geq k }$.
% \begin{definition}[Precedence constrained worst case active learning (PCWCAL)]
% Given a set $\cH$ of $n$ hypotheses, a set $\cT$ of $m$ tests, and a DAG $\cF = \brc{\cT, \preceq}$ encoding precedence constraints, construct a decision tree respecting precedence constraints that identifies any hypothesis from $\cH$ while minimizing the worst-case depth of the tree.
% \end{definition}
% \begin{definition}[Precedence constrained average case active learning (PCACAL)]
% Given a set $\cH$ of $n$ hypotheses, a set $\cT$ of $m$ tests and a DAG $\cF = \brc{\cT, \preceq}$ encoding precedence constraints,
% % and a probability distribution over $\cH$,
% construct a decision tree respecting precedence constraints that identifies any hypothesis from $\cH$ while minimizing the expected depth (average case cost).
% \end{definition}

\medskip
\paraTitle{Other notation.}
Suppose that a search strategy performed some tests $T\subseteq\cT$.
If a hypothesis $h\in\cH$ has the property for each test $t\in T$ the reply is a set $U_t\in t$ such that $h\in U_t$, then we cay that $h$ is a \emph{potential target}.
Denote by $\cH_T$ the set of all potential targets after the tests in $T$.
Note that $\cH_T$ does not depend on the order of performing these tests.
If $v$ is any node of a decision tree $D$ then $\cH_v$ is defined similarly to be all hypotheses that are potential targets after the tests $\tests{D}{v}$.

\medskip
We finish this section with a few simple facts.
\begin{observation}
    If $D$ is a decision tree for a $\ProblemPCAL$ instance with hypotheses $\cH$, then:
    $$
    \COSTA\br{D} = \sum_{t \in \inner{D}} \spr{\cH_t}.
    $$
\end{observation}
\DD{unifikacja $I$ vs $\cI$}

The next lemma gives a `monotonicity' property for $\ProblemPCAL$ with respect to considering subproblems.
%We will also make use of the following folklore lemmas which are due to the fact that any decision tree for $I$ can be restricted to a decision tree for each subproblem $\br{\cH_i, \cT, \cF}$ without increasing the cost:

\begin{lemma}\label{lemma:subspace_opt}
    Let $\cI=\br{\cH, \cT,\cF}$ and $\cI'=\br{\cH', \cT, \cF}$ be two instances of $\ProblemPCAL$, where $\cH'\subseteq \cH$.
    Then, $\OPT\br{\cI'} \leq \OPT\br{\cI}$.
\end{lemma}
\begin{lemma}\label{lemma:subinstances_avg}
    Let $I=(\cH, \cT, \cF)$ be an instance of $\ProblemPCAL$.
    Let $\cH_1,\dots,\cH_t\subseteq \cH$ be such that $\bigcup_{i=1}^t \cH_i \subseteq \cH$ and for any $i\neq j$, $\cH_i\cap \cH_j = \emptyset$.
    Then,
    $$
    \OPTA\br{\cI} \geq \sum_{i=1}^t \OPTA\br{\br{\cH_i, \cT, \cF}}.
    $$
\end{lemma}



