\section{Preliminaries}
% \begin{definition}[Precedence constrained set cover (PCSC)]
% Given a universe $\mathcal{U}$ of $n$ items, a collection $\mathcal{S}$ of $m$ subsets of $\mathcal{U}$, a DAG $\cF = \brc{\mathcal{S}, \preceq}$ encoding precedence constraints, and a coverage requirement $K$, find a precedence-closed subfamily $\mathcal{C} \subseteq \mathcal{S}$ that covers at least $K$ items while minimizing $\spr{\mathcal{C}}$.
% \end{definition}
% \begin{definition}[Precedence constrained min-sum set cover (PCMSSC)]
% Given a universe $\mathcal{U}$ of $n$ items, a collection $\mathcal{S}$ of $m$ subsets of $\mathcal{U}$, a DAG $\cF = \brc{\mathcal{S}, \preceq}$ encoding precedence constraints, and a coverage requirement $K$, find a precedence-closed sequence of sets that covers at least $K$ items while minimizing the average time (position in sequence) at which items are covered.
% \end{definition}

% In the active learning problems, we are given a set of hypotheses $\cH$ and a set of tests $\cT$. Each test $t$ is an arbitrary partition of $\cH$ into disjoint subsets $U_{t,1}, U_{t,2}, \ldots, U_{t,r_t}$, where $r_t$ is the number of possible responses to test $t$. When a test $t$ is performed, the response indicates which subset $U_{t,j}$ contains the hidden hypothesis $\target \in \cH$. The tests are subject to precedence constraints encoded by a DAG $\cF = \brc{\cT, \preceq}$, meaning that a test $t$ can be performed only if all its predecessors in $\cF$ have already been performed.

\paraTitle{Decision trees.}
In all adaptive learning problems, the goal is to design a \textit{strategy} $\cS$ understood as an adaptive algorithm, that based on the previous replies, gives \questioner the next test to be performed.
This strategy is typically represented as a \textit{decision tree}, a rooted tree $D$ in which internal nodes represent tests to be performed, edges represent replies and each leaf represents a hypothesis.
Formally, a decision tree is defined recursively as follows.
If $r=\brc{R_1,\ldots,R_l}\in\cT$ is the first test performed by $\cS$, then $r$ is the root of $D$.
For each reply $R_i$, $i\in\brc{1,\ldots,l}$, take inductively defined decision tree $D_i$ derived from the queries done by $\cS$ when the reply to $r$ is $R_i$.
Then, $D$ is obtained by making the roots of $D_1,\ldots,D_l$ the children of $r$, and the edge connecting $r$ to the root of $D_i$ is labeled with the reply $R_i$.
To complete the inductive definition, if $\cS$ is a trivial strategy that outputs the target, then the corresponding tree $D$ is a leaf labeled with the target.
It should be remarked that it is possible for a test to appear multiple times in the decision tree.
However, with a slight abuse of notation we use tests as nodes of $D$ whenever there is no ambiguity.

For any node $v$ of $D$, $D_v$ is the subtree of $D$ that consists of $v$ and all its descendants.

\medskip
\paraTitle{Cost measures.}
Let $\tests{D}{v}$, denote the sequence of tests performed in order to arrive at the node $v$ in $D$, including $v$.
In other words, $\tests{D}{v}$ consists of the nodes of the path from the root to $v$ in $D$.
The cost of identifying a hypothesis $h$ using a decision tree $D$ is $\COST\br{D, h}=\spr{\tests{D}{v}}-1$, where $v$ is the leaf of $D$ that represent the event of declaring that $h$ is the target.
We consider two cost measures for decision trees: the worst-case cost $\COSTW\br{D} = \max_{h \in \cH} \COST\br{D, h}$ and the average-case cost $\COSTA\br{D} = \sum_{h \in \cH} \COST\br{D, h}$ (up to a multiplicative factor of $1/\spr{\cH}$).
The criteria we consider for any $\ProblemPCAL$ instance $I$ are denoted by
$$\OPTA\br{\cI}=\min\brc{\COSTA\br{D} \mid D\textup{ is a decision tree}},$$
$$\OPTW\br{\cI}=\min\brc{\COSTW\br{D} \mid D\textup{ is a decision tree}}.$$
%\DD{Skoro mamy ju≈º te problemy $\ProblemPCWCAL$ oraz $\ProblemPCACAL$ zdefiniowane, to tutaj aby nie powtarzac w nowej notacji drze decyzyjnych zamienie to na uwage/obserwacje w ramach jednego paragrafu do optymalizacji czego to sie sprowadza odnosnie drzew dec.}
Hence the $\ProblemPCACAL$ aims at finding a decision tree $D$ such that $\COSTA\br{D}=\OPTA\br{\cI}$ and in case of $\ProblemPCWCAL$ the goal is to find $D$ such that $\COSTW\br{D}=\OPTW\br{\cI}$.

Denote by $\optPCSC{\cI}$, for an instance $\cI$, the size of a set $\cC$ that is a valid solution to $\ProblemPCSC$ and has minimum size.
For a sequence $\cC$ being a solution to $\ProblemPCMSSC$, we use the symbol $\coverageTime\br{\cC}$ to denote the sum of coverage times of all elements in $\bigcup_{S\in \cC} S$ (with a slight abuse of notation we use the set notaion for a sequence).
Then, for an instance $\cI=\br{\cU,\cS,\preceq,k}$ of $\ProblemPCMSSC$ define $\optPCMSSC{\cI}=\min\brc{\coverageTime\br{\bigcup_{S\in\cC}} \mid \spr{\bigcup_{S\in\cC} S}\geq k\textup{ and $S$ is precedence closed} }$.
% \begin{definition}[Precedence constrained worst case active learning (PCWCAL)]
% Given a set $\cH$ of $n$ hypotheses, a set $\cT$ of $m$ tests, and a DAG $\cF = \brc{\cT, \preceq}$ encoding precedence constraints, construct a decision tree respecting precedence constraints that identifies any hypothesis from $\cH$ while minimizing the worst-case depth of the tree.
% \end{definition}
% \begin{definition}[Precedence constrained average case active learning (PCACAL)]
% Given a set $\cH$ of $n$ hypotheses, a set $\cT$ of $m$ tests and a DAG $\cF = \brc{\cT, \preceq}$ encoding precedence constraints,
% % and a probability distribution over $\cH$,
% construct a decision tree respecting precedence constraints that identifies any hypothesis from $\cH$ while minimizing the expected depth (average case cost).
% \end{definition}

\medskip
\paraTitle{Approximation measures.}
We say that an algoritm is $(\gamma,\alpha)$-approximation for $\ProblemPCSC$ (respectively $\ProblemPCMSSC$), where $\gamma\geq 1$ and $\alpha\geq 1$, if for any input instance $\cI=(\cU,\cS,\preceq,k)$ it returns a solution $\cC$ that covers at least $k/\alpha$ items and $\spr{\cC}\leq\gamma\cdot\optPCSC{\cI}$ (resectively $\coverageTime\br{\bigcup_{S\in\cC}S}\leq\gamma\cdot\optPCMSSC{\cI}$).

\medskip
\paraTitle{Other notation.}
Suppose that a search strategy performed some tests $T\subseteq\cT$.
If a hypothesis $h\in\cH$ has the property for each test $t\in T$ the reply is a set $U_t\in t$ such that $h\in U_t$, then we cay that $h$ is a \emph{potential target}.
Denote by $\cH_T$ the set of all potential targets after the tests in $T$.
Note that $\cH_T$ does not depend on the order of performing these tests.
Likewise, if $v$ is any node of a decision tree $D$ then $\cH_v$ consists of all hypotheses that are potential targets after the tests $\tests{D}{v}$.

\medskip
We finish this section with a few simple facts.
\begin{observation}
    If $D$ is a decision tree for a $\ProblemPCAL$ instance with hypotheses $\cH$, then:
    $$
    \COSTA\br{D} = \sum_{t \in \inner{D}} \spr{\cH_t},
    $$
    where $\inner{D}$ is the set of non-leaf vertices of $D$.
\end{observation}
\begin{lemma}\label{lemma:subspace_opt}
    Let $\cI=\br{\cH, \cT,\preceq}$ and $\cI'=\br{\cH', \cT, \preceq}$ be two instances of $\ProblemPCAL$, where $\cH'\subseteq \cH$.
    Then, $\OPT\br{\cI'} \leq \OPT\br{\cI}$.
\end{lemma}
\begin{lemma}\label{lemma:subinstances_avg}
    Let $\cI=(\cH, \cT, \preceq)$ be an instance of $\ProblemPCAL$.
    Let $\cH_1,\dots,\cH_t\subseteq \cH$ be such that $\bigcup_{i=1}^t \cH_i \subseteq \cH$ and for any $i\neq j$, $\cH_i\cap \cH_j = \emptyset$.
    Then,
    $$
    \OPTA\br{\cI} \geq \sum_{i=1}^t \OPTA\br{\br{\cH_i, \cT, \preceq}}.
    $$
\end{lemma}



